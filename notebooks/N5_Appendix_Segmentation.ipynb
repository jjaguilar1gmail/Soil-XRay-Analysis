{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7362b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xray_stats import load_process as lp\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "import scipy.optimize as opt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.ndimage import generic_filter\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb040e",
   "metadata": {},
   "source": [
    "# X-Ray Segmentation\n",
    "In this notebook, I provide an example of experimental segmentation performed to identify soil vs non-soil pixels. The technique I use is indicator kriging based segmentation technique as developed by Oh and Lindquist (1999)\n",
    "\n",
    "Oh, Wonho, and Brent Lindquist. \"Image thresholding by indicator kriging.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 21.7 (1999): 590-602.\n",
    "\n",
    "The first two cells below are the functions needed to segment a soil scan. After that, the remaining cells (a) segment a sample soil scan, (b) visualize the \"void\" voxels in 2D, and (c) clear the memory - important if you want to re-run without a memory crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b9e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cdf_fast(array, num_array, bins=10000):\n",
    "    \"\"\"\n",
    "    Calculate the Cumulative Distribution Function (CDF) for a list of numbers in a numpy array.\n",
    "    This version uses a histogram and interpolation for speed.\n",
    "\n",
    "    Parameters:\n",
    "    array: numpy array\n",
    "    num_array: list or numpy array of numbers for which CDF is calculated\n",
    "    bins: number of bins to use in the histogram\n",
    "\n",
    "    Returns:\n",
    "    cdf_values: list of floats\n",
    "        The CDF values for the numbers\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the histogram\n",
    "    counts, bin_edges = np.histogram(array, bins=bins, density=True)\n",
    "\n",
    "    # Calculate the CDF\n",
    "    cdf = np.cumsum(counts)\n",
    "    cdf = cdf / cdf[-1]  # normalize to 1\n",
    "\n",
    "    # Use interpolation to find the CDF values for num_array\n",
    "    cdf_values = np.interp(num_array, bin_edges[1:], cdf)\n",
    "\n",
    "    return cdf_values\n",
    "\n",
    "# Load all PILs\n",
    "def tiff_stack_array(stack_index):\n",
    "    total_images, tiff_files_sorted, path_to_tiff_stack = lp.get_tiff_stack(stack_index)\n",
    "    tiff_stack = []\n",
    "    for tiff_index in range(total_images):\n",
    "        tiff_stack.append(Image.open(os.path.join(path_to_tiff_stack, tiff_files_sorted[tiff_index])))\n",
    "    return total_images, tiff_stack\n",
    "\n",
    "# Convert a subset to np arrays\n",
    "def get_tiff_stack_np_array(tiff_stack, begin=None, end=None):\n",
    "    if begin is None and end is None:\n",
    "        np_array = np.stack([np.array(img) for img in tiff_stack])\n",
    "    else:\n",
    "        np_array = np.stack([np.array(img) for img in tiff_stack[begin:end]])\n",
    "    return np_array\n",
    "\n",
    "# Get the values needed to normalize by container density, as well as the centers for masking core later\n",
    "def get_norm_and_core_centers(tiff_stack):\n",
    "    \n",
    "    tiff_total = len(tiff_stack)\n",
    "    \n",
    "    # ignore the top and bottom edges of the tiff stack\n",
    "    low_tiff = round(tiff_total*0.1)\n",
    "    high_tiff = round(tiff_total*0.9)\n",
    "    \n",
    "    # arrays to collect core center for whole stack interpolation\n",
    "    interp_inds = []\n",
    "    xcs = []\n",
    "    ycs = []\n",
    "    \n",
    "    # arrays to calculate median outside and container attenuations for normalization\n",
    "    outer_noise_attenuation = [];\n",
    "    container_attenuation = [];\n",
    "    \n",
    "    i = -1;\n",
    "    subset_indices = np.linspace(low_tiff, high_tiff - 1, num=50, dtype=int)\n",
    "    \n",
    "    # Subsample the tiffs to get average of background noise and container density\n",
    "    for tiff_index in subset_indices:\n",
    "        i = i+1\n",
    "        image = np.array(tiff_stack[tiff_index])\n",
    "        mask_container, mask_outside, mask_core = lp.get_masks_pre(image)\n",
    "        xc, yc, r = lp.estimate_circle_from_image_pre(image)\n",
    "        interp_inds.append(tiff_index)\n",
    "        xcs.append(xc)\n",
    "        ycs.append(yc)\n",
    "        \n",
    "        outer_noise_attenuation.append(np.nanmean(image[mask_outside]))\n",
    "        container_attenuation.append(np.nanmean(image[mask_container]))\n",
    "        lp.print_progress(i, len(subset_indices))\n",
    "        \n",
    "    outer_median = np.median(outer_noise_attenuation)\n",
    "    container_median = np.median(container_attenuation)\n",
    "    fx = interp1d(interp_inds,xcs,fill_value=\"extrapolate\")\n",
    "    fy = interp1d(interp_inds,ycs,fill_value=\"extrapolate\")\n",
    "    \n",
    "    xc_s = fx(list(range(0, tiff_total)))\n",
    "    yc_s = fy(list(range(0, tiff_total)))\n",
    "    \n",
    "    return outer_median, container_median, xc_s, yc_s\n",
    "\n",
    "# Normalize image by container density\n",
    "def scale_and_norm(np_image, outer_median, container_median):\n",
    "    return (np_image-outer_median)/(container_median-outer_median)*1.022\n",
    "  \n",
    "## Thresholding\n",
    "# Thresholding - smoothed indicator function (eq 21 in Oh and Lindquist)\n",
    "def get_threshold_masks_soft_global(image,T0,T1,sample_set=None):\n",
    "    \n",
    "    if sample_set is None:\n",
    "        sample_set = image\n",
    "        \n",
    "    # Standard deviations of the thresholded populations\n",
    "    sig_0 = np.std(sample_set[sample_set<T0])\n",
    "    sig_1 = np.std(sample_set[sample_set>T1])\n",
    "    \n",
    "    s0_l = 0\n",
    "    s1_r = 0\n",
    "    \n",
    "    s0_r = (sig_0*T1+sig_1*T0)/(sig_0+sig_1)\n",
    "    s1_l = (sig_0*T1+sig_1*T0)/(sig_0+sig_1)\n",
    "    \n",
    "    # cdfs for soft indicators\n",
    "    F_lims = calculate_cdf_fast(sample_set.flatten(), [T0-s0_l,T0+s0_r,T1-s1_l,T1+s1_r], bins=10000)\n",
    "    F_low_softs = calculate_cdf_fast(sample_set.flatten(), image[(image>=(T0-s0_l))&(image<=(T0+s0_r))], bins=10000)\n",
    "    F_high_softs = calculate_cdf_fast(sample_set.flatten(), image[(image>=(T1-s1_l))&(image<=(T1+s1_r))], bins=10000)\n",
    "    \n",
    "    # void/air (0) threshold indicator\n",
    "    i_T0 = image*0;\n",
    "    i_T0[image<(T0-s0_l)] = 1\n",
    "    i_T0[image>(T0+s0_r)] = 0\n",
    "    i_T0[(image>=(T0-s0_l))&(image<=(T0+s0_r))] = (F_lims[1]-F_low_softs)/(F_lims[1]-F_lims[0])\n",
    "    \n",
    "    # solid/soil (1) threshold indicator\n",
    "    i_T1 = image*0;\n",
    "    i_T1[image<(T1-s1_l)] = 1\n",
    "    i_T1[image>(T1+s1_r)] = 0\n",
    "    i_T1[(image>=(T1-s1_l))&(image<=(T1+s1_r))] = (F_lims[3]-F_high_softs)/(F_lims[3]-F_lims[2])\n",
    "    \n",
    "    return i_T0, i_T1, F_lims, s0_l, s1_r, s0_r, s1_l\n",
    "\n",
    "def get_threshold_masks_soft(image,T0,T1,F_lims, s0_l, s1_r, s0_r, s1_l,sample_set=None):\n",
    "    \n",
    "    if sample_set is None:\n",
    "        sample_set = image\n",
    "\n",
    "    F_low_softs = calculate_cdf_fast(sample_set.flatten(), image[(image>=(T0-s0_l))&(image<=(T0+s0_r))], bins=100)\n",
    "    F_high_softs = calculate_cdf_fast(sample_set.flatten(), image[(image>=(T1-s1_l))&(image<=(T1+s1_r))], bins=100)\n",
    "    \n",
    "    # void/air (0) threshold indicator\n",
    "    i_T0 = image*0;\n",
    "    i_T0[image<(T0-s0_l)] = 1\n",
    "    i_T0[image>(T0+s0_r)] = 0\n",
    "    i_T0[(image>=(T0-s0_l))&(image<=(T0+s0_r))] = (F_lims[1]-F_low_softs)/(F_lims[1]-F_lims[0])\n",
    "    \n",
    "    # solid/soil (1) threshold indicator\n",
    "    i_T1 = image*0;\n",
    "    i_T1[image<(T1-s1_l)] = 1\n",
    "    i_T1[image>(T1+s1_r)] = 0\n",
    "    i_T1[(image>=(T1-s1_l))&(image<=(T1+s1_r))] = (F_lims[3]-F_high_softs)/(F_lims[3]-F_lims[2])\n",
    "    \n",
    "    return i_T0, i_T1\n",
    "\n",
    "# Kriging\n",
    "    # Build Array\n",
    "def get_neighbor_coordinates(r):\n",
    "    \"\"\"\n",
    "    Generate coordinates within a radius r.\n",
    "\n",
    "    Parameters:\n",
    "    r: int\n",
    "        Radius for the generation of coordinates.\n",
    "\n",
    "    Returns:\n",
    "    neighbor_coordinates: numpy array\n",
    "        An array of tuples with the coordinates within the given radius.\n",
    "    \"\"\"\n",
    "    neighbor_coordinates = np.array([(dx, dy) for dx in range(-r, r+1) for dy in range(-r, r+1) if dx**2 + dy**2 <= r**2])\n",
    "    # Find the index of [0, 0]\n",
    "    index = np.where((neighbor_coordinates == [0, 0]).all(axis=1))[0]\n",
    "    # Delete [0, 0] from the current position and append it at the end\n",
    "    neighbor_coordinates = np.delete(neighbor_coordinates, index, axis=0)\n",
    "    neighbor_coordinates = np.append(neighbor_coordinates, [[0, 0]], axis=0)\n",
    "    return neighbor_coordinates\n",
    "  \n",
    "def create_kernel(neighbor_coordinates, weights):\n",
    "    \"\"\"\n",
    "    Create a kernel matrix based on the extents of the coordinates and assign weights.\n",
    "\n",
    "    Parameters:\n",
    "    neighbor_coordinates: numpy array\n",
    "        An array of tuples with the coordinates.\n",
    "    weights: numpy array\n",
    "        An array of weights with the same length as neighbor_coordinates.\n",
    "\n",
    "    Returns:\n",
    "    kernel: numpy array\n",
    "        A kernel matrix with weights assigned to corresponding neighbor_coordinates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the minimum and maximum coordinates in each dimension\n",
    "    min_coords = np.min(neighbor_coordinates, axis=0)\n",
    "    max_coords = np.max(neighbor_coordinates, axis=0)\n",
    "    \n",
    "    # The size of the kernel is the difference between the min and max coordinates, plus 1\n",
    "    # We add 1 because the coordinates are 0-indexed\n",
    "    kernel_size = max_coords - min_coords + 1\n",
    "    \n",
    "    # Create the empty kernel\n",
    "    kernel = np.zeros(kernel_size, dtype=float)\n",
    "    \n",
    "    # Shift the coordinates so that the minimum coordinate is at (0, 0)\n",
    "    shifted_coordinates = neighbor_coordinates - min_coords\n",
    "    \n",
    "    # Assign the weights to the corresponding cells in the kernel\n",
    "    for coord, weight in zip(shifted_coordinates, weights):\n",
    "        kernel[tuple(coord)] = weight\n",
    "    \n",
    "    return kernel\n",
    "\n",
    "def calculate_average_empirical_semivariogram(xray_set, max_lag_distance=12):\n",
    "    semis = [];\n",
    "    for i in range(xray_set.shape[0]):\n",
    "        semis.append(calculate_empirical_semivariogram(xray_set[i], max_lag_distance=max_lag_distance))\n",
    "    empirical_semivariogram = np.mean(semis,axis=0)\n",
    "    return empirical_semivariogram\n",
    "\n",
    "def calculate_empirical_semivariogram(xray_data, max_lag_distance=12):\n",
    "    # Flattening the 2D data to 1D\n",
    "    data_1d = xray_data.flatten()\n",
    "\n",
    "    # Getting the coordinates of the voxels\n",
    "    y,x = np.indices(xray_data.shape)\n",
    "    coordinates = np.vstack([x.flatten(), y.flatten()]).T\n",
    "\n",
    "    # Calculating the distances and differences between every pair of voxels\n",
    "    distances = pdist(coordinates, metric='euclidean')\n",
    "    differences = pdist(data_1d[:, None], metric='euclidean')\n",
    "\n",
    "    # Only considering pairs within the max lag distance\n",
    "    mask = distances <= max_lag_distance\n",
    "\n",
    "    distances = distances[mask]\n",
    "    differences = differences[mask]\n",
    "\n",
    "    # Binning the distances and differences into the lags\n",
    "    bins = np.arange(0, max_lag_distance+1)\n",
    "    bin_indices = np.digitize(distances, bins)\n",
    "\n",
    "    # Calculating the empirical semivariogram\n",
    "    empirical_semivariogram = np.array([np.mean(differences[bin_indices == i]**2) / 2.0 for i in range(1, len(bins))])\n",
    "    empirical_semivariogram[0] = 0;\n",
    "    return empirical_semivariogram\n",
    "\n",
    "def fit_semivariogram_model(empirical_semivariogram, model, initial_guess):\n",
    "    # Define a cost function for the residual error minimization problem\n",
    "    def cost_function(params):\n",
    "        return np.sum((empirical_semivariogram - model(np.arange(len(empirical_semivariogram)), *params))**2)\n",
    "\n",
    "    # Use simulated annealing to find the optimal parameters\n",
    "    result = opt.basinhopping(cost_function, initial_guess, niter=1000)\n",
    "\n",
    "    return result.x\n",
    "\n",
    "# Define a model for the semivariogram, wer are using gaussian\n",
    "def varimodel(h, nugget, sill, range):\n",
    "    # Gaussian model\n",
    "    return nugget + sill * (1 - np.exp(-(h / range) ** 2))\n",
    "\n",
    "def solve_kriging(CC):\n",
    "    \"\"\"\n",
    "    Solve the Indicator Kriging system of equations.\n",
    "\n",
    "    Parameters:\n",
    "    CC: numpy array of shape (n+1, n+1)\n",
    "        Covariance matrix where the last row and last column (except the very last element) \n",
    "        represent covariances between the observed locations and the estimation location, \n",
    "        and the rest of the matrix represents covariances between the observed locations.\n",
    "\n",
    "    Returns:\n",
    "    weights: numpy array of shape (n,)\n",
    "        Weights assigned to each observed value.\n",
    "    lagrange_multiplier: float\n",
    "        Lagrange multiplier enforcing the unbiasedness condition.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the covariances between the observed locations\n",
    "    C = CC[:-1, :-1]\n",
    "    # Extract the covariances between the observed locations and the estimation location\n",
    "    c = CC[:-1, -1]\n",
    "    # Add a row and a column of ones to the matrix, and a 1 at the end of the vector\n",
    "    # This is for the Lagrange multiplier that enforces the unbiasedness condition\n",
    "    C = np.vstack((C, np.ones(C.shape[1])))\n",
    "    C = np.hstack((C, np.ones((C.shape[0], 1))))\n",
    "    c = np.append(c, 1)\n",
    "\n",
    "    # Solve the kriging system\n",
    "    #solution = np.linalg.solve(C, c)\n",
    "    solution, residuals, rank, s = np.linalg.lstsq(C, c, rcond=None)\n",
    "\n",
    "    # The weights are all but the last element of the solution\n",
    "    weights = solution[:-1]\n",
    "    \n",
    "    # Negative weights in the solution of (7) have the potential of producing negative probabilities in (6).\n",
    "    # If negative weights occur, we adjust the weights using the simple, robust, a posteriori scheme proposed in [29].   \n",
    "    # Let $x_1, ..., p$ denote the subset of locations where the weights are negative, \n",
    "    indices = np.where(weights < 0)\n",
    "     \n",
    "    if len(indices)>0:\n",
    "        # $\\bar{w}$ denote the average magnitude of the negative weights, and $C$ denote the average covariance.\n",
    "        negabsmean = abs(np.nanmean(weights[indices]))\n",
    "        negcmean = np.nanmean(c[indices])\n",
    "        # Positive weights smaller than $\\bar{w}$ whose covariance\n",
    "        # to the location $x_0$ is smaller than $C$ are also set to zero.\n",
    "        weights[(weights>0)&(weights<negabsmean)&(weights<negcmean)]=0\n",
    "        # The negative weights are set to zero.\n",
    "        weights[indices]\n",
    "    # The remaining positive weights are renormalized uniformly to sum to one.\n",
    "    weights = weights/np.sum(weights)\n",
    "        \n",
    "    # The last element of the solution is the Lagrange multiplier\n",
    "    lagrange_multiplier = solution[-1]\n",
    "\n",
    "    return weights\n",
    "def majority_filter(image, size):\n",
    "    \"\"\"\n",
    "    Apply a majority filter to an thresholded image. This image should have one of three values:\n",
    "    0: pixels in the original image less than a lower threshold\n",
    "    1: pixels in the original image greater than an upper threshold\n",
    "    2: unclassified pixels - these are ignored\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    image: numpy array\n",
    "        The input image.\n",
    "    size: int\n",
    "        The size of the filter. Must be odd.\n",
    "\n",
    "    Returns:\n",
    "    numpy array\n",
    "        The filtered image.\n",
    "    \"\"\"\n",
    "    # Define the majority function\n",
    "    def majority(x):\n",
    "        center_pixel = x[len(x) // 2]\n",
    "        output_pixel = center_pixel\n",
    "        if center_pixel==0:\n",
    "            if (np.sum(x==1)/len(x))>=0.6:\n",
    "                output_pixel=5\n",
    "        elif center_pixel==1:\n",
    "            if (np.sum(x==0)/len(x))>=0.6:\n",
    "                output_pixel=-5\n",
    "        # Return the value that occurs most frequently\n",
    "        return output_pixel\n",
    "\n",
    "    # Apply the majority filter\n",
    "    return generic_filter(image, majority, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def krig_image(tiff_stack, tiff_int, T0, T1, outer_median, container_median, xc_s, yc_s, T0_kernal, T1_kernal, F_lims, s0_l, s1_r, s0_r, s1_l, sample_set):\n",
    "    # Process a sample image\n",
    "    curim = np.array(tiff_stack[tiff_int])\n",
    "    curim = scale_and_norm(curim, outer_median, container_median) ## ALWAYS REMEMBER TO SCALE\n",
    "\n",
    "    # calculate the softened indicator values for the sample set\n",
    "    i_T0, i_T1 = get_threshold_masks_soft(curim,T0,T1,F_lims, s0_l, s1_r, s0_r, s1_l, sample_set)\n",
    "    curim_init = ((curim>=T0)&(curim<=T1))*2 + (curim<T0)*0 + (curim>T1)*1\n",
    "\n",
    "    # Get your unknown population\n",
    "    ui,uj = np.where((curim>=T0)&(curim<=T1))\n",
    "\n",
    "    #P0 = convolve2d(i_T0, T0_kernal,mode='same')\n",
    "    #P1 = convolve2d(i_T1, T1_kernal,mode='same')\n",
    "    \n",
    "    P0 = convolve(i_T0, T0_kernal)\n",
    "    P1 = convolve(i_T1, T1_kernal)\n",
    "    \n",
    "    Kval = (1-P1)>=P0\n",
    "    curim_kriged = curim_init.copy()\n",
    "    curim_kriged[ui,uj] = Kval[ui,uj]\n",
    "    \n",
    "    mask_container, mask_outside, mask_core = lp.masks_by_region(curim_kriged, xc_s[tiff_int], yc_s[tiff_int],490)\n",
    "    voids_y, voids_x = np.where((curim_kriged==0)&mask_core)\n",
    "    voids_z = voids_y*0-tiff_int\n",
    "    \n",
    "    return voids_x,voids_y,voids_z\n",
    "\n",
    "def krig_image_show(tiff_stack, tiff_int, T0, T1, outer_median, container_median, xc_s, yc_s, T0_kernal, T1_kernal, F_lims, s0_l, s1_r, s0_r, s1_l, sample_set):\n",
    "    # Process a sample image\n",
    "    curim = np.array(tiff_stack[tiff_int])\n",
    "    #curim = scale_and_norm(denoise_tv(curim), outer_median, container_median) ## ALWAYS REMEMBER TO SCALE\n",
    "    curim = scale_and_norm((curim), outer_median, container_median) ## ALWAYS REMEMBER TO SCALE\n",
    "\n",
    "    # calculate the softened indicator values for the sample set\n",
    "    i_T0, i_T1 = get_threshold_masks_soft(curim,T0,T1,F_lims, s0_l, s1_r, s0_r, s1_l, sample_set)\n",
    "    curim_init = ((curim>=T0)&(curim<=T1))*2 + (curim<T0)*0 + (curim>T1)*1\n",
    "\n",
    "    # Get your unknown population\n",
    "    ui,uj = np.where((curim>=T0)&(curim<=T1))\n",
    "\n",
    "    #P0 = convolve2d(i_T0, T0_kernal,mode='same')\n",
    "    #P1 = convolve2d(i_T1, T1_kernal,mode='same')\n",
    "    \n",
    "    P0 = convolve(i_T0, T0_kernal)\n",
    "    P1 = convolve(i_T1, T1_kernal)\n",
    "    KP = np.abs((1-P1)-P0)<0.05\n",
    "    Kval = (1-P1)>=P0\n",
    "    curim_kriged = curim_init.copy()\n",
    "    curim_kriged[ui,uj] = Kval[ui,uj]\n",
    "    \n",
    "    mask_container, mask_outside, mask_core = lp.masks_by_region(curim_kriged, xc_s[tiff_int], yc_s[tiff_int],490)\n",
    "    voids_y, voids_x = np.where((curim_kriged==0)&mask_core)\n",
    "    voids_z = voids_y*0-tiff_int\n",
    "    \n",
    "    fig, ax = plt.subplots(4,1,figsize=(15, 60))\n",
    "    ax[0].imshow(curim,cmap=\"gray\",vmin=0,vmax=2.5)\n",
    "    draw_circle(xc_s[tiff_int], yc_s[tiff_int], 490, ax[0])\n",
    "    ax[1].imshow(curim_init)\n",
    "    draw_circle(xc_s[tiff_int], yc_s[tiff_int], 490, ax[1])\n",
    "    ax[2].imshow(curim_kriged)\n",
    "    draw_circle(xc_s[tiff_int], yc_s[tiff_int], 490, ax[2])\n",
    "    ax[3].imshow(KP,cmap=\"gray\")\n",
    "    print(np.min(KP))\n",
    "    print(np.max(KP))\n",
    "    draw_circle(xc_s[tiff_int], yc_s[tiff_int], 490, ax[3])\n",
    "    return voids_x,voids_y,voids_z\n",
    "\n",
    "def get_voids(stack_int):\n",
    "    # our kriging window is a radius of 3 around unlabelled pixel\n",
    "    neighbor_coordinates = get_neighbor_coordinates(3)\n",
    "    # Threshold selection\n",
    "    T0 = 0.5\n",
    "    T1 = 1.075\n",
    "    print('load tiff stack')\n",
    "    # Load a tiff stack\n",
    "    if 'tiff_stack' in locals():\n",
    "        del tiff_stack\n",
    "    total_images, tiff_stack = tiff_stack_array(stack_int)\n",
    "    # Get its median outer and container attenuations for normalization\n",
    "    print('calculate normalization parameters and get interpolated core centers')\n",
    "    outer_median, container_median, xc_s, yc_s = get_norm_and_core_centers(tiff_stack)\n",
    "    # Sample voxel set for building semivariograms (a central subset from 20 sample images from index 0 to 200 or first 7.8 mm).\n",
    "    sample_set = np.stack([np.array(img) for img in tiff_stack[0:200:20]])\n",
    "    sample_set = sample_set[:,700:800,700:800]\n",
    "    sample_set = scale_and_norm(sample_set, outer_median, container_median) ## ALWAYS REMEMBER TO SCALE\n",
    "    print('calculating softened indicator values')\n",
    "    # calculate the softened indicator values for the sample set\n",
    "    i_T0, i_T1, F_lims, s0_l, s1_r, s0_r, s1_l = get_threshold_masks_soft_global(sample_set,T0,T1)\n",
    "    print('calculating semivariograms')\n",
    "    # calculate a model for all images in sample_set and average together.\n",
    "    T0_semivariogram = calculate_average_empirical_semivariogram(i_T0,12)\n",
    "    T0_params = fit_semivariogram_model(T0_semivariogram, varimodel, initial_guess=[0.0, 1, 10.0])\n",
    "\n",
    "    # calculate a model for all images in sample_set and average together.\n",
    "    T1_semivariogram = calculate_average_empirical_semivariogram(i_T1,12)\n",
    "    T1_params = fit_semivariogram_model(T1_semivariogram, varimodel, initial_guess=[0.0, 1, 10.0])\n",
    "    coordinates = neighbor_coordinates.copy()\n",
    "    # Calculate the distances between all pairs of voxels\n",
    "    distances = squareform(pdist(coordinates)) \n",
    "\n",
    "    print('calculating T0 and T1 gaussian semivariogram models')\n",
    "    # Calculate the semivariogram for these distances\n",
    "    T0_model = varimodel(distances,*T0_params)\n",
    "    T1_model = varimodel(distances,*T1_params)\n",
    "    print('calculating T0 and T1 covariances')\n",
    "    # Calculate the covariance matrices\n",
    "    T0_covariance = T0_params[1] - T0_model\n",
    "    T1_covariance = T1_params[1] - T1_model\n",
    "    print('calculating kriging weights')\n",
    "    # Solve for weights\n",
    "    T0_weights = solve_kriging(T0_covariance)\n",
    "    T1_weights = solve_kriging(T1_covariance)\n",
    "\n",
    "    T0_kernal = create_kernel(coordinates[:-1], T0_weights)\n",
    "    T1_kernal = create_kernel(coordinates[:-1], T1_weights)\n",
    "    \n",
    "    voids_x = [];\n",
    "    voids_y = [];\n",
    "    voids_z = [];\n",
    "    \n",
    "    for tiff_int in range(total_images):\n",
    "        void_x,void_y,void_z = krig_image(tiff_stack,tiff_int, T0, T1, outer_median, container_median, xc_s, yc_s, T0_kernal, T1_kernal,F_lims, s0_l, s1_r, s0_r, s1_l, sample_set)\n",
    "        voids_x.append(void_x)\n",
    "        voids_y.append(void_y)\n",
    "        voids_z.append(void_z)\n",
    "        lp.print_progress(tiff_int,total_images)\n",
    "    del tiff_stack\n",
    "    voids_x = np.concatenate(voids_x)\n",
    "    voids_y = np.concatenate(voids_y)\n",
    "    voids_z = np.concatenate(voids_z)\n",
    "    return voids_x,voids_y,voids_z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba237f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "voids_x,voids_y,voids_z = get_voids(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c337dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas DataFrame from the NumPy arrays\n",
    "df = pd.DataFrame({'x': voids_x, 'y': voids_y, 'z': voids_z})\n",
    "# Create a canvas using Datashader\n",
    "cvs = ds.Canvas(plot_width=(np.max(voids_x)-np.min(voids_x))//2, plot_height=np.abs(np.min(voids_z))//2, x_range=(np.min(voids_x), np.max(voids_x)),\n",
    "                y_range=(np.min(voids_z), np.max(voids_z)))\n",
    "\n",
    "# Aggregate the points into a 2D heatmap using the y coordinate as the quantity\n",
    "agg = cvs.points(df, 'x', 'z', ds.count())\n",
    "\n",
    "# Convert the aggregated heatmap to a NumPy array\n",
    "heatmap = tf.shade(agg)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "# Plot the heatmap\n",
    "heatmap_np = np.asarray(np.log1p(agg))\n",
    "#ax.imshow(heatmap_np, cmap='hot', origin='lower',  extent=[np.min(voids_x), np.max(voids_x),\n",
    "                            #                            np.min(voids_z), np.max(voids_z)])\n",
    "# Add colorbar, xlabel, ylabel, and title\n",
    "cbar = plt.colorbar(ax.imshow(heatmap_np, cmap='hot', origin='lower',extent=[np.min(voids_x), np.max(voids_x),\n",
    "                                                                               np.min(voids_z), np.max(voids_z)]),\n",
    "                    ax=ax, label='Voxel Quantity')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Z')\n",
    "ax.set_title('Voxel Quantity Heatmap')\n",
    "\n",
    "heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clear up memory\n",
    "del voids_x, voids_y, voids_z, df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c442aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
