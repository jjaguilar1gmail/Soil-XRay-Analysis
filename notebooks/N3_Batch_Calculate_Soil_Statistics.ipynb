{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7780c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xray_stats import load_process as lp\n",
    "from xray_stats import df_plotting as dp\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714c20a",
   "metadata": {},
   "source": [
    "## Batch Calculate Soil Statistics\n",
    "**Summary:** In this Jupyter notebook, we calculate local and coarse statistics for all x-ray datasets and save them to a csv file for subsequent analysis and processing. These metrics include per-horizontal-slice calculations of **mean, median, standard deviation, 5th percentile and 95th percentile** of **sliding-window skewness, sliding-window kurtosis, sliding-window variance, sobel edges, pixel intensity and density**. Take a look at the load_process library (**get_stats** function in particular along with the functions it calls) along with previous notebooks for an overview of these calculations.\n",
    "\n",
    "First we introduce the function that calculates these metrics, example usage, and an explanation for the choice of parameters used in a precomputed dataset that we use in later analyses. Then, we provide code compiling statistics for all horizontal slices in a scan as well by binned depths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd4cbf",
   "metadata": {},
   "source": [
    "### Compute per slice metrics\n",
    "\n",
    "Here we provide code for compiling statistics into a dataframe and saving to a csv file. The full dataset has already been computed and provided in ../data/precomputed_soil_stats.csv. In the following cell we provide an example for computing a subset along with the settings and parameters needed for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7c099c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       stack_index  tiff_index  block  sub-rep  window_size   skip  \\\n",
      "count        136.0   136.00000  136.0    136.0   136.000000  136.0   \n",
      "mean          10.0  1675.00000    2.0      1.0   100.000000   10.0   \n",
      "std            0.0   985.01739    0.0      0.0    50.184844    0.0   \n",
      "min           10.0     0.00000    2.0      1.0    50.000000   10.0   \n",
      "25%           10.0   837.50000    2.0      1.0    50.000000   10.0   \n",
      "50%           10.0  1675.00000    2.0      1.0   100.000000   10.0   \n",
      "75%           10.0  2512.50000    2.0      1.0   150.000000   10.0   \n",
      "max           10.0  3350.00000    2.0      1.0   150.000000   10.0   \n",
      "\n",
      "        skew_mean  skew_median    skew_std     skew_p5  ...   img_median  \\\n",
      "count  136.000000   136.000000  136.000000  136.000000  ...   136.000000   \n",
      "mean    -0.993965    -1.047691    1.431102   -3.085000  ...  6698.112472   \n",
      "std      0.564904     0.574676    0.606970    1.015630  ...   605.882269   \n",
      "min     -2.335720    -2.419290    0.447662   -5.136869  ...  4569.546503   \n",
      "25%     -1.348257    -1.380904    1.002445   -3.823447  ...  6528.889274   \n",
      "50%     -0.959689    -1.013277    1.437686   -3.263051  ...  6901.941245   \n",
      "75%     -0.678694    -0.792151    1.658418   -2.315888  ...  7075.763690   \n",
      "max      0.766256     0.743320    4.543854   -0.175002  ...  7320.108509   \n",
      "\n",
      "           img_std       img_p5      img_p95  img_mean_norm (g/cm3)  \\\n",
      "count   136.000000   136.000000   136.000000             136.000000   \n",
      "mean    574.821863  5591.067477  7277.066955               1.931413   \n",
      "std     308.650891  1266.125017   266.487779               0.238593   \n",
      "min     239.136345  2951.080192  6552.493330               1.348764   \n",
      "25%     293.025917  4140.753815  7190.648914               1.795947   \n",
      "50%     430.175255  6373.210273  7270.521757               2.059359   \n",
      "75%     897.741630  6623.149749  7437.510975               2.090978   \n",
      "max    1214.534100  6896.548841  7738.178901               2.454285   \n",
      "\n",
      "       img_median_norm (g/cm3)  img_std_norm (g/cm3)  img_p5_norm (g/cm3)  \\\n",
      "count               136.000000            136.000000           136.000000   \n",
      "mean                  1.970570              0.254498             1.478648   \n",
      "std                   0.227800              0.140595             0.526230   \n",
      "min                   1.283991              0.101338             0.550174   \n",
      "25%                   1.910822              0.122621             0.853476   \n",
      "50%                   2.066499              0.192328             1.828531   \n",
      "75%                   2.103332              0.396935             1.900540   \n",
      "max                   2.454656              0.563783             2.291906   \n",
      "\n",
      "       img_p95_norm (g/cm3)       depth  \n",
      "count            136.000000  136.000000  \n",
      "mean               2.225310    6.532500  \n",
      "std                0.092848    3.841568  \n",
      "min                1.993860    0.000000  \n",
      "25%                2.198027    3.266250  \n",
      "50%                2.248753    6.532500  \n",
      "75%                2.266812    9.798750  \n",
      "max                2.616362   13.065000  \n",
      "\n",
      "[8 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "stacks = [10] # List of xray scan indices to compute between 1 and 54 corresponding to \"Sample ID\" in the meta.csv.\n",
    "winds = [50,150] # List of square window sizes in pixels to use for sliding window computations\n",
    "denoises = [True] # Denoising is always important for reducing noise that may affect sliding window and sobel computations\n",
    "tiffskips = 50 # Number of horizontal slices to skip to reduce dataset and computation time.\n",
    "pixelskips = 10 # The number of rows and columns to skip between sliding window calculations.\n",
    "\n",
    "datas = []\n",
    "for stack in stacks:\n",
    "    # Build the array of tiff indices to sample.\n",
    "    total_images, tiff_files_sorted, path_to_tiff_stack = lp.get_tiff_stack(stack)\n",
    "    tiffs = np.arange(0, total_images, tiffskips)\n",
    "    for tiff in tiffs:\n",
    "        for wind in winds:\n",
    "            for denoise in denoises:\n",
    "                lp.print_and_flush(f\"Stack: {stack}, Tiff: {tiff}, Window Size: {wind}, Denoise: {denoise}\")\n",
    "                try:\n",
    "                    datas.append(lp.get_stats(tiff,stack,wind,pixelskips,denoise,False,tiff_files_sorted, path_to_tiff_stack))\n",
    "                except:\n",
    "                    print(\"something went wrong\")\n",
    "datas = pd.concat(datas, axis=0)\n",
    "datas = datas.reset_index(drop=True)\n",
    "datas.to_csv(\"../data/sample_subset_stats.csv\")\n",
    "print(datas.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770b772",
   "metadata": {},
   "source": [
    "**NOTE ON SCAN (STACK) 30:** The images from these scans are missing the container and outside air pixels, as such one must exclude this scan from per treatment statistical analyses of density. However, it can still be used for sliding window soil heterogeneity statistics.\n",
    "\n",
    "### Selecting reasonable skip values\n",
    "Skip parameters for both pixels and tiffs were selected such that calculation time was reduced without largely affecting statistics. Below are some analyses for different skip values. After calculating statistics, we also employ another useful library module included: **df_plotting**. This module simply takes a pandas dataframe and desired columns to use for data filtering and defaults for plotting, and produces an interactive gui for plotting 2D and 3D scatter plots as well as boxplots. This is useful for visualization aspects of exploratory data analysis (EDA).\n",
    "\n",
    "#### Varying Pixel Skips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a55f4e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack: 23, Tiff: 3000, Window Size: 50, Pixel Skip: 100    \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db920981b47e414083990a1e3fc825ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Dropdown(description='X-axis:', index=9, layout=Layout(width='90%â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Varying Pixel Skips - Only affects statistics of sliding window metrics\n",
    "\n",
    "stacks = [23] # List of xray scan indices to compute between 1 and 54 corresponding to \"Sample ID\" in the meta.csv.\n",
    "winds = [50] # List of square window sizes in pixels to use for sliding window computations\n",
    "denoises = [True] # Denoising is always important for reducing noise that may affect sliding window and sobel computations\n",
    "tiffskips = 500 # Number of horizontal slices to skip to reduce dataset and computation time.\n",
    "pixelskips = [5, 10, 20, 50, 100] # The number of rows and columns to skip between sliding window calculations.\n",
    "\n",
    "datas = []\n",
    "for stack in stacks:\n",
    "    # Build the array of tiff indices to sample.\n",
    "    total_images, tiff_files_sorted, path_to_tiff_stack = lp.get_tiff_stack(stack)\n",
    "    tiffs = np.arange(0, total_images, tiffskips)\n",
    "    for tiff in tiffs:\n",
    "        for wind in winds:\n",
    "            for pixelskip in pixelskips:\n",
    "                lp.print_and_flush(f\"Stack: {stack}, Tiff: {tiff}, Window Size: {wind}, Pixel Skip: {pixelskip}    \")\n",
    "                try:\n",
    "                    datas.append(lp.get_stats(tiff,stack,wind,pixelskip,denoises[0],False,tiff_files_sorted, path_to_tiff_stack))\n",
    "                except:\n",
    "                    print(\"something went wrong\")\n",
    "datas = pd.concat(datas, axis=0)\n",
    "datas = datas.reset_index(drop=True)\n",
    "\n",
    "\n",
    "filter_cols = [\"stack_index\",\"window_size\",\"tillage\",\"fertilizer\",\"tillage-fertilizer\",\"denoise\"]\n",
    "default_cols = [\"skip\",\"kurt_mean\",\"edge_mean\",\"depth\",\"stack_index\",\"tillage\"]\n",
    "gui = dp.build_gui(datas,filter_cols,default_cols)\n",
    "display(gui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685fcf46",
   "metadata": {},
   "source": [
    "**Results of varying how many pixels (image rows/columns) to skip when collecting sliding window calculations:** An analysis on one scan indicates that skipping more than approximately every 20 pixels begins to impact the resulting average sliding window metrics. This is particularly true for a sliding window size of 50x50 pixels. This effect is reduced for larger window sizes, however a window size of 50 yields useful insights, thus, to be safe, we use a piixel skip value of 10.\n",
    "\n",
    "#### Varying Tiff Skips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98457e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using every 10 tiffs (horizontal x-ray slices)kip: 10    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f40462ec6241a7b0fa6999d09e64ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Dropdown(description='X-axis:', index=9, layout=Layout(width='90%â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using every 50 tiffs (horizontal x-ray slices)kip: 50    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13783b5a08b2477193e71a7f4e595a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Dropdown(description='X-axis:', index=9, layout=Layout(width='90%â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using every 100 tiffs (horizontal x-ray slices)ip: 100    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4def8f2e844a299febc551edb4c2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Dropdown(description='X-axis:', index=9, layout=Layout(width='90%â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using every 300 tiffs (horizontal x-ray slices)ip: 300    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9821dc3848947ed9ddc79c1e7885f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Dropdown(description='X-axis:', index=9, layout=Layout(width='90%â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Varying Tiff Skips - Affects statistics of all metrics\n",
    "\n",
    "stacks = [23] # List of xray scan indices to compute between 1 and 54 corresponding to \"Sample ID\" in the meta.csv.\n",
    "winds = [50] # List of square window sizes in pixels to use for sliding window computations\n",
    "denoises = [True] # Denoising is always important for reducing noise that may affect sliding window and sobel computations\n",
    "tiffskips = [10,50,100,300] # Number of horizontal slices to skip to reduce dataset and computation time.\n",
    "pixelskips = [10] # The number of rows and columns to skip between sliding window calculations.\n",
    "\n",
    "guis = [];\n",
    "for tiffskip in tiffskips:\n",
    "    datas = []\n",
    "    for stack in stacks:\n",
    "        # Build the array of tiff indices to sample.\n",
    "        total_images, tiff_files_sorted, path_to_tiff_stack = lp.get_tiff_stack(stack)\n",
    "        tiffs = np.arange(0, total_images, tiffskip)\n",
    "        for tiff in tiffs:\n",
    "            for wind in winds:\n",
    "                for pixelskip in pixelskips:\n",
    "                    lp.print_and_flush(f\"Stack: {stack}, Tiff: {tiff}, Window Size: {wind}, Tiff Skip: {tiffskip}    \")\n",
    "                    try:\n",
    "                        datas.append(lp.get_stats(tiff,stack,wind,pixelskip,denoises[0],False,tiff_files_sorted, path_to_tiff_stack))\n",
    "                    except:\n",
    "                        print(\"something went wrong\")\n",
    "    datas = pd.concat(datas, axis=0)\n",
    "    datas = datas.reset_index(drop=True)\n",
    "    filter_cols = [\"stack_index\",\"window_size\",\"tillage\",\"fertilizer\",\"tillage-fertilizer\",\"denoise\"]\n",
    "    default_cols = [\"skip\",\"kurt_mean\",\"edge_mean\",\"tiff_index\",\"stack_index\",\"tillage\"]\n",
    "    gui = dp.build_gui(datas,filter_cols,default_cols)\n",
    "    print(f\"Using every {tiffskip} tiffs (horizontal x-ray slices)\")\n",
    "    display(gui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4734f961",
   "metadata": {},
   "source": [
    "**Results:** While using very sparse datasets seemed to have minimal impact on metrics averaged across the entire depth (since we are still sampling across the entire depth range regardless), having a tiff skip value of at most 50 is still useful to generate reliable statistics for binned depths.\n",
    "\n",
    "### Parameters for full dataset:\n",
    "After some preliminary testing, below are the parameters used to generate statistics on the entire dataset. We collect data on different sliding window sizes, although 50x50 produced useful results. This cell has already been run and with the statistics saved as \"../data/precomputed_soil_stats.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f92fdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack: 30, Tiff: 0, Window Size: 100, Denoise: Truerue\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffreyaguilar/Documents/Danforth/DensityProject/XRayAnalysis/src/xray_stats/load_process.py:576: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in double_scalars\n",
      "\n",
      "/Users/jeffreyaguilar/Documents/Danforth/DensityProject/XRayAnalysis/src/xray_stats/load_process.py:579: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in double_scalars\n",
      "\n",
      "/Users/jeffreyaguilar/Documents/Danforth/DensityProject/XRayAnalysis/src/xray_stats/load_process.py:582: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in double_scalars\n",
      "\n",
      "/Users/jeffreyaguilar/Documents/Danforth/DensityProject/XRayAnalysis/src/xray_stats/load_process.py:585: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in double_scalars\n",
      "\n",
      "/Users/jeffreyaguilar/Documents/Danforth/DensityProject/XRayAnalysis/src/xray_stats/load_process.py:588: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in double_scalars\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack: 54, Tiff: 3300, Window Size: 150, Denoise: True\r"
     ]
    }
   ],
   "source": [
    "stacks = list(range(54))\n",
    "stacks = [x + 1 for x in stacks]\n",
    "winds = [50,100,150]\n",
    "denoises = [True]\n",
    "tiffskips = 50\n",
    "pixelskips = 10\n",
    "\n",
    "datas = []\n",
    "for stack in stacks:\n",
    "    # Build the array of tiff indices to sample.\n",
    "    total_images, tiff_files_sorted, path_to_tiff_stack = lp.get_tiff_stack(stack)\n",
    "    tiffs = np.arange(0, total_images, tiffskips)\n",
    "    for tiff in tiffs:\n",
    "        for wind in winds:\n",
    "            for denoise in denoises:\n",
    "                lp.print_and_flush(f\"Stack: {stack}, Tiff: {tiff}, Window Size: {wind}, Denoise: {denoise}\")\n",
    "                try:\n",
    "                    datas.append(lp.get_stats(tiff,stack,wind,pixelskips,denoise,False,tiff_files_sorted, path_to_tiff_stack))\n",
    "                except:\n",
    "                    print(\"something went wrong\")\n",
    "datas = pd.concat(datas, axis=0)\n",
    "datas = datas.reset_index(drop=True)\n",
    "datas.to_csv('../data/full_soil_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea9be8",
   "metadata": {},
   "source": [
    "With the full dataset now batch calculated and saved, the next two cells generate bulk statistics both by scan, and by scan with depth bins. In these cells, we assume that there are **multiple *window sizes*, only one *pixel skip* value, and *denoise* always True**. If this changes, you must modify these cells to accurately compute bulk statistics. These two cells have already been run with csv's saved as \"../data/precomputed_soil_stats_depth_binned.csv\" and \"\"../data/precomputed_soil_stats_compiled.csv\" (only one bin encompassing entire depth range). In the next notebook we will begin to explore this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a20e924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate Bulk Statistics for each x-ray soil core scan across 4 depth bins.\n",
    "\n",
    "original_df = pd.read_csv('../data/precomputed_soil_stats.csv')\n",
    "new_df = []\n",
    "\n",
    "# Drop rows with missing values in the 'depth' column\n",
    "original_df = original_df.dropna(subset=['depth'])\n",
    "depth_bins = pd.cut(original_df['depth'], bins=4)\n",
    "for stack_index, window_size in original_df[['stack_index', 'window_size']].drop_duplicates().itertuples(index=False):\n",
    "    for depths in depth_bins.drop_duplicates():\n",
    "        subset_df = original_df[(original_df['stack_index'] == stack_index) & (original_df['window_size'] == window_size) & (depth_bins == depths)]\n",
    "        subset_df.reset_index()\n",
    "        # Extract the unique values from the subset_df (assuming they are the same within each subset)\n",
    "        row = subset_df[['stack_index', 'file_name', 'tillage', 'fertilizer', 'tillage-fertilizer',\n",
    "                                       'block', 'sub-rep', 'window_size', 'skip', 'denoise']]\n",
    "        row = row.head(1)\n",
    "        # Calculate mean, median, and std of specified columns\n",
    "        for column in ['skew_mean', 'skew_median', 'skew_std', 'skew_p5',\n",
    "                       'skew_p95', 'kurt_mean', 'kurt_median', 'kurt_std', 'kurt_p5',\n",
    "                       'kurt_p95', 'vari_mean', 'vari_median', 'vari_std', 'vari_p5',\n",
    "                       'vari_p95', 'edge_mean', 'edge_median', 'edge_std', 'edge_p5',\n",
    "                       'edge_p95', 'img_mean', 'img_median', 'img_std', 'img_p5', 'img_p95',\n",
    "                       'img_mean_norm (g/cm3)', 'img_median_norm (g/cm3)',\n",
    "                       'img_std_norm (g/cm3)', 'img_p5_norm (g/cm3)', 'img_p95_norm (g/cm3)']:\n",
    "            row[f'{column}_mean'] = subset_df[f'{column}'].mean()\n",
    "            row[f'{column}_median'] = subset_df[f'{column}'].median()\n",
    "            row[f'{column}_std'] = subset_df[f'{column}'].std()\n",
    "    \n",
    "        # Calculate mean, max, and min of depth and tiff_index\n",
    "        row[f'mean_depth'] = subset_df['depth'].mean()\n",
    "        row[f'max_depth'] = subset_df['depth'].max()\n",
    "        row[f'min_depth'] = subset_df['depth'].min()\n",
    "        row[f'mean_tiff_index'] = subset_df['tiff_index'].mean()\n",
    "        row[f'max_tiff_index'] = subset_df['tiff_index'].max()\n",
    "        row[f'min_tiff_index'] = subset_df['tiff_index'].min()\n",
    "\n",
    "        # Append the row to the new_df\n",
    "        new_df.append(pd.DataFrame(row))\n",
    "\n",
    "new_df = pd.concat(new_df,axis=0)\n",
    "new_df.reset_index()\n",
    "# Export the new_df to CSV\n",
    "new_df.to_csv('../data/full_soil_stats_depth_binned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed74f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate Bulk Statistics for each x-ray soil core scan across 1 single depth bin encompassing all depths.\n",
    "\n",
    "original_df = pd.read_csv('../data/precomputed_soil_stats.csv')\n",
    "new_df = []\n",
    "\n",
    "# Drop rows with missing values in the 'depth' column\n",
    "original_df = original_df.dropna(subset=['depth'])\n",
    "#depth_bins = pd.cut(original_df['depth'], bins=4)\n",
    "for stack_index, window_size in original_df[['stack_index', 'window_size']].drop_duplicates().itertuples(index=False):\n",
    "    subset_df = original_df[(original_df['stack_index'] == stack_index) & (original_df['window_size'] == window_size)]\n",
    "    subset_df.reset_index()\n",
    "    # Extract the unique values from the subset_df (assuming they are the same within each subset)\n",
    "    row = subset_df[['stack_index', 'file_name', 'tillage', 'fertilizer', 'tillage-fertilizer',\n",
    "                                   'block', 'sub-rep', 'window_size', 'skip', 'denoise']]\n",
    "    row = row.head(1)\n",
    "    # Calculate mean, median, and std of specified columns\n",
    "    for column in ['skew_mean', 'skew_median', 'skew_std', 'skew_p5',\n",
    "                   'skew_p95', 'kurt_mean', 'kurt_median', 'kurt_std', 'kurt_p5',\n",
    "                   'kurt_p95', 'vari_mean', 'vari_median', 'vari_std', 'vari_p5',\n",
    "                   'vari_p95', 'edge_mean', 'edge_median', 'edge_std', 'edge_p5',\n",
    "                   'edge_p95', 'img_mean', 'img_median', 'img_std', 'img_p5', 'img_p95',\n",
    "                   'img_mean_norm (g/cm3)', 'img_median_norm (g/cm3)',\n",
    "                   'img_std_norm (g/cm3)', 'img_p5_norm (g/cm3)', 'img_p95_norm (g/cm3)']:\n",
    "        row[f'{column}_mean'] = subset_df[f'{column}'].mean()\n",
    "        row[f'{column}_median'] = subset_df[f'{column}'].median()\n",
    "        row[f'{column}_std'] = subset_df[f'{column}'].std()\n",
    "\n",
    "    # Calculate mean, max, and min of depth and tiff_index\n",
    "    row[f'mean_depth'] = subset_df['depth'].mean()\n",
    "    row[f'max_depth'] = subset_df['depth'].max()\n",
    "    row[f'min_depth'] = subset_df['depth'].min()\n",
    "    row[f'mean_tiff_index'] = subset_df['tiff_index'].mean()\n",
    "    row[f'max_tiff_index'] = subset_df['tiff_index'].max()\n",
    "    row[f'min_tiff_index'] = subset_df['tiff_index'].min()\n",
    "\n",
    "    # Append the row to the new_df\n",
    "    new_df.append(pd.DataFrame(row))\n",
    "\n",
    "new_df = pd.concat(new_df,axis=0)\n",
    "new_df.reset_index()\n",
    "# Export the new_df to CSV\n",
    "new_df.to_csv('../data/full_soil_stats_compiled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e75ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
